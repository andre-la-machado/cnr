{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this Notebook, a LSTM Model will be tried for the Competitition Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set_style('darkgrid')\n",
    "from cnr_methods import get_selected_features, transform_data, revert_data,metric_cnr, get_simplified_data\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the data used correspond to the results of the Feature Engineering and Selection Step. For simplicity, during Hyperparameter Optimization, only Wind Farm 3 Training Data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = get_selected_features(0)\n",
    "\n",
    "#full_data = full_data.rename({'Unnamed: 0' : 'Time'},axis=1)\n",
    "full_data = full_data.set_index('Time')\n",
    "\n",
    "full_label = pd.read_csv('Data/Y_train.csv')\n",
    "X = full_data[full_data['Set']=='Train']\n",
    "\n",
    "WF = 'WF3'\n",
    "X = X[X['WF']==WF]\n",
    "y = full_label[full_label['ID'].isin(X['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['ID','WF','Set'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                       U_100m    V_100m     U_10m     V_10m        T  \\\nTime                                                                   \n2018-05-01 01:00:00  5.789500  3.820200  1.054669  1.317597  275.690   \n2018-05-01 02:00:00  6.003300  3.920600  0.876879  1.483483  275.770   \n2018-05-01 03:00:00  5.931829  0.907656  0.949640  1.419591  276.875   \n2018-05-01 04:00:00  5.205300  1.683800  1.027462  1.029786  275.650   \n2018-05-01 05:00:00  4.845900  0.702200  1.011645  0.785352  275.530   \n\n                          CLCT  Wind Direction 100m  Wind Direction 10m  \\\nTime                                                                      \n2018-05-01 01:00:00  86.504507             0.583268            0.895782   \n2018-05-01 02:00:00  98.976088             0.578533            1.036951   \n2018-05-01 03:00:00  64.193607             0.151837            0.981212   \n2018-05-01 04:00:00  57.482484             0.312855            0.786528   \n2018-05-01 05:00:00  89.971463             0.143904            0.660129   \n\n                     Wind Speed 100m  V_100m_lag_21_days  ...  \\\nTime                                                      ...   \n2018-05-01 01:00:00         6.936299                 NaN  ...   \n2018-05-01 02:00:00         7.170127                 NaN  ...   \n2018-05-01 03:00:00         6.000870                 NaN  ...   \n2018-05-01 04:00:00         5.470862                 NaN  ...   \n2018-05-01 05:00:00         4.896512                 NaN  ...   \n\n                     V_10m_Rolling_14_Window_Variance  \\\nTime                                                    \n2018-05-01 01:00:00                               NaN   \n2018-05-01 02:00:00                               NaN   \n2018-05-01 03:00:00                               NaN   \n2018-05-01 04:00:00                               NaN   \n2018-05-01 05:00:00                               NaN   \n\n                     U_100m_Rolling_14_Window_Mean  Wind Speed 10m  \\\nTime                                                                 \n2018-05-01 01:00:00                            NaN        1.687717   \n2018-05-01 02:00:00                            NaN        1.723264   \n2018-05-01 03:00:00                            NaN        1.707938   \n2018-05-01 04:00:00                            NaN        1.454695   \n2018-05-01 05:00:00                            NaN        1.280704   \n\n                     V_100m_Rolling_7_Window_Mean  T_lag_21_days  \\\nTime                                                               \n2018-05-01 01:00:00                           NaN            NaN   \n2018-05-01 02:00:00                           NaN            NaN   \n2018-05-01 03:00:00                           NaN            NaN   \n2018-05-01 04:00:00                           NaN            NaN   \n2018-05-01 05:00:00                           NaN            NaN   \n\n                     U_10m_Rolling_14_Window_Variance  \\\nTime                                                    \n2018-05-01 01:00:00                               NaN   \n2018-05-01 02:00:00                               NaN   \n2018-05-01 03:00:00                               NaN   \n2018-05-01 04:00:00                               NaN   \n2018-05-01 05:00:00                               NaN   \n\n                     CLCT_Rolling_7_Window_Mean  V_100m_lag_7_days  \\\nTime                                                                 \n2018-05-01 01:00:00                         NaN                NaN   \n2018-05-01 02:00:00                         NaN                NaN   \n2018-05-01 03:00:00                         NaN                NaN   \n2018-05-01 04:00:00                         NaN                NaN   \n2018-05-01 05:00:00                         NaN                NaN   \n\n                     U_10m_lag_7_days  U_100m_lag_21_days  \nTime                                                       \n2018-05-01 01:00:00               NaN                 NaN  \n2018-05-01 02:00:00               NaN                 NaN  \n2018-05-01 03:00:00               NaN                 NaN  \n2018-05-01 04:00:00               NaN                 NaN  \n2018-05-01 05:00:00               NaN                 NaN  \n\n[5 rows x 93 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>U_100m</th>\n      <th>V_100m</th>\n      <th>U_10m</th>\n      <th>V_10m</th>\n      <th>T</th>\n      <th>CLCT</th>\n      <th>Wind Direction 100m</th>\n      <th>Wind Direction 10m</th>\n      <th>Wind Speed 100m</th>\n      <th>V_100m_lag_21_days</th>\n      <th>...</th>\n      <th>V_10m_Rolling_14_Window_Variance</th>\n      <th>U_100m_Rolling_14_Window_Mean</th>\n      <th>Wind Speed 10m</th>\n      <th>V_100m_Rolling_7_Window_Mean</th>\n      <th>T_lag_21_days</th>\n      <th>U_10m_Rolling_14_Window_Variance</th>\n      <th>CLCT_Rolling_7_Window_Mean</th>\n      <th>V_100m_lag_7_days</th>\n      <th>U_10m_lag_7_days</th>\n      <th>U_100m_lag_21_days</th>\n    </tr>\n    <tr>\n      <th>Time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-05-01 01:00:00</th>\n      <td>5.789500</td>\n      <td>3.820200</td>\n      <td>1.054669</td>\n      <td>1.317597</td>\n      <td>275.690</td>\n      <td>86.504507</td>\n      <td>0.583268</td>\n      <td>0.895782</td>\n      <td>6.936299</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.687717</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 02:00:00</th>\n      <td>6.003300</td>\n      <td>3.920600</td>\n      <td>0.876879</td>\n      <td>1.483483</td>\n      <td>275.770</td>\n      <td>98.976088</td>\n      <td>0.578533</td>\n      <td>1.036951</td>\n      <td>7.170127</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.723264</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 03:00:00</th>\n      <td>5.931829</td>\n      <td>0.907656</td>\n      <td>0.949640</td>\n      <td>1.419591</td>\n      <td>276.875</td>\n      <td>64.193607</td>\n      <td>0.151837</td>\n      <td>0.981212</td>\n      <td>6.000870</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.707938</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 04:00:00</th>\n      <td>5.205300</td>\n      <td>1.683800</td>\n      <td>1.027462</td>\n      <td>1.029786</td>\n      <td>275.650</td>\n      <td>57.482484</td>\n      <td>0.312855</td>\n      <td>0.786528</td>\n      <td>5.470862</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.454695</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 05:00:00</th>\n      <td>4.845900</td>\n      <td>0.702200</td>\n      <td>1.011645</td>\n      <td>0.785352</td>\n      <td>275.530</td>\n      <td>89.971463</td>\n      <td>0.143904</td>\n      <td>0.660129</td>\n      <td>4.896512</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.280704</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 93 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(input_shape):\n",
    "  # Numerical branch\n",
    "\n",
    "  input_layer = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "  hidden_1 = tf.keras.layers.LSTM(units=256, activation=\"relu\", kernel_initializer=\"he_normal\")(input_layer)\n",
    "  hidden_1 = tf.keras.layers.Dropout(rate=0.5)(hidden_1)\n",
    "  hidden_1 = tf.keras.layers.BatchNormalization()(hidden_1)\n",
    "\n",
    "  # Output\n",
    "\n",
    "  outputs = tf.keras.layers.PReLU()(hidden_1)\n",
    "  outputs = tf.keras.layers.Dropout(rate=0.5)(outputs)\n",
    "  outputs = tf.keras.layers.BatchNormalization()(outputs)\n",
    "  outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(outputs)\n",
    "\n",
    "  model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM_Model((X.shape[0],X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 6239, 93)]        0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 93)                69564     \n_________________________________________________________________\ndropout (Dropout)            (None, 93)                0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 93)                372       \n_________________________________________________________________\np_re_lu (PReLU)              (None, 93)                93        \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 93)                0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 93)                372       \n_________________________________________________________________\ndense (Dense)                (None, 1)                 94        \n=================================================================\nTotal params: 70,495\nTrainable params: 70,123\nNon-trainable params: 372\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(317)\n",
    "tf.random.set_seed(317)\n",
    "\n",
    "patience = 10\n",
    "epochs = 40\n",
    "total_folds = 5\n",
    "total_it = 120\n",
    "monitor = \"val_accuracy\"\n",
    "batch_size = 1 * ((len(X) - len(X) // total_folds) // (total_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Define Time Split Cross Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=k_fold_splits)\n",
    "\n",
    "    # Separating Data from Hold Out Set\n",
    "\n",
    "    X_cv, _, y_cv, _ = train_test_split(X, y, test_size=0.125, shuffle=False)\n",
    "\n",
    "    train_scores = np.empty(0)\n",
    "    val_scores = np.empty(0)\n",
    "    test_scores = np.empty(0)\n",
    "    for train_index, test_index in tscv.split(X_cv):\n",
    "\n",
    "        # Get the Data of the Split\n",
    "        X_train, X_test = X_cv.iloc[train_index], X_cv.iloc[test_index]\n",
    "        y_train, y_test = y_cv.iloc[train_index], y_cv.iloc[test_index]\n",
    "\n",
    "        # Separating Training Set of Split on Train and Validation Subsets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=0, min_delta=1e-8)]\n",
    "\n",
    "        # Train the Model\n",
    "        model.compile(optimizer='adam', loss='hinge', metrics=[\"accuracy\"])\n",
    "        history = model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, y_val), callbacks=callbacks_list)\n",
    "\n",
    "        # Train and Validation Score\n",
    "        #train_score = np.array(progress['train']['CAPE']).mean()\n",
    "        #val_score = np.array(progress['eval']['CAPE']).mean()\n",
    "\n",
    "        # Test Score\n",
    "        preds = model.predict(X_test,batch_size = batch_size,callbacks=callbacks_list)\n",
    "        test_score = metric_cnr(preds,X_test)\n",
    "\n",
    "        #train_scores = np.append(train_scores,train_score)\n",
    "        #val_scores = np.append(val_scores,val_score)\n",
    "        test_scores = np.append(test_scores,test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold Out Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.125, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594766366661",
   "display_name": "Python 3.7.6 64-bit ('andre_': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}