{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective fo this notebook is to utilize some methods to find and determine the optimal model to solve the challenge problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cnr_methods import get_selected_features, transform_data, revert_data,metric_cnr, get_simplified_data\n",
    "import tsfresh\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from collections import deque\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, full data is read and the column names are changed to avoid errors on LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = get_selected_features(500)\n",
    "\n",
    "full_data = full_data.rename({'Unnamed: 0' : 'Time'},axis=1)\n",
    "full_data = full_data.set_index('Time')\n",
    "\n",
    "full_label = pd.read_csv('Data/Y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "columns = []\n",
    "for column in full_data.columns:\n",
    "    column = re.sub(r'[^a-zA-Z0-9]', '',column)\n",
    "    columns.append(column)\n",
    "full_data.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the data used correspond to the results of the Feature Engineering and Selection Step. For simplicity, during Hyperparameter Optimization, only Wind Farm 3 Training Data is used. Wind Farm 3 is selected here because it has the greatest Production, which is a positive thing for the Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data[full_data['Set']=='Train']\n",
    "WF = 'WF3'\n",
    "X = X[X['WF']==WF]\n",
    "y = full_label[full_label['ID'].isin(X['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['ID','WF','Set'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, differentiation (numpy.diff) is applied to the data, before the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transform_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the Hyperparameter search, it is necessary first to have some way to reliably measure the performance of the model. For this purpose, it will be used a Time Split Cross Validation Method, were the \"Test\" Fold for each Iteration is going to be used as the Validation Data, and so, to make Early Stopping on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_splits = 5\n",
    "num_boost_round = 1000\n",
    "early_stopping_rounds = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(param,k_fold_splits=k_fold_splits,num_boost_round=num_boost_round,early_stopping_rounds=early_stopping_rounds):\n",
    "    # Define Time Split Cross Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=k_fold_splits)\n",
    "\n",
    "    # Separating Data from Hold Out Set\n",
    "\n",
    "    X_cv, _, y_cv, _ = train_test_split(X, y, test_size=0.125, shuffle=False)\n",
    "\n",
    "    # Set Objective Functions\n",
    "    param['objective'] = 'mean_absolute_percentage_error'\n",
    "    param['metric'] = 'mean_absolute_percentage_error'\n",
    "\n",
    "\n",
    "    test_scores = np.empty(0)\n",
    "    for train_index, test_index in tscv.split(X_cv):\n",
    "\n",
    "        # Get the Data of the Split\n",
    "        X_train, X_test = X_cv.iloc[train_index], X_cv.iloc[test_index]\n",
    "        y_train, y_test = y_cv.iloc[train_index], y_cv.iloc[test_index]\n",
    "\n",
    "        # Separating Training Set of Split on Train and Validation Subsets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)\n",
    "\n",
    "        # Apply GPU-DF Transformation\n",
    "        dtrain = lgb.Dataset(X_train,label=y_train['Production'])\n",
    "        dval = lgb.Dataset(X_val,label=y_val['Production'])\n",
    "        dtest = lgb.Dataset(X_test,label=y_test['Production'],free_raw_data=False).construct()\n",
    "\n",
    "        # Train the Model\n",
    "        progress = dict()\n",
    "        bst = lgb.train(param, dtrain, num_boost_round=num_boost_round, valid_sets=[dval], valid_names=['eval'], early_stopping_rounds=early_stopping_rounds,verbose_eval=False,evals_result=progress)\n",
    "\n",
    "        # Test Score\n",
    "        preds = bst.predict(dtest.get_data(),num_iteration=bst.best_iteration)\n",
    "        test_score = metric_cnr(preds,dtest)\n",
    "\n",
    "        test_scores = np.append(test_scores,test_score[1])\n",
    "\n",
    "    return {'loss' : test_scores.mean(), 'params' : param, 'status' : STATUS_OK, 'test_score_array' : test_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] Objective Function with Train, Validation and Test Sets Only (No CV, Not being used at the Moment)\n",
    "\n",
    "def objective_2(param,k_fold_splits=k_fold_splits,num_boost_round=num_boost_round,early_stopping_rounds=early_stopping_rounds):\n",
    "    # Define Time Split Cross Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=k_fold_splits)\n",
    "\n",
    "    # Separating Data from Hold Out Set\n",
    "    X_cv, X_hold, y_cv, y_hold = train_test_split(X, y, test_size=0.125, shuffle=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cv, y_cv, test_size=0.125, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, shuffle=False)\n",
    "\n",
    "    # Apply GPU-DF Transformation\n",
    "    dtrain = lgb.Dataset(X_train,label=y_train['Production'])\n",
    "    dval = lgb.Dataset(X_val,label=y_val['Production'])\n",
    "    dtest = lgb.Dataset(X_test,label=y_test['Production'],free_raw_data=False).construct()\n",
    "\n",
    "    # Set Objective Functions\n",
    "    param['objective'] = 'mean_absolute_percentage_error'\n",
    "    param['metric'] = 'mean_absolute_percentage_error'\n",
    "\n",
    "    progress = dict()\n",
    "    bst = lgb.train(param, dtrain, num_boost_round=num_boost_round, valid_sets=[dval], valid_names=['eval'], early_stopping_rounds=early_stopping_rounds,verbose_eval=False,evals_result=progress)\n",
    "\n",
    "    # Test Score\n",
    "    preds = bst.predict(dtest.get_data(),num_iteration=bst.best_iteration)\n",
    "    #preds = np.clip(preds,0,None)\n",
    "    test_score = metric_cnr(preds,dtest)\n",
    "\n",
    "    return {'loss' : test_score[1], 'params' : param, 'status' : STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hyperparameter Tuning, the HyperOpt Library will be used, which implements some techniques for a more efficient search for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'max_depth' : 5 + hp.randint('max_depth', 30),\n",
    "    'bagging_fraction' : hp.uniform('bagging_fraction', 0.5, 1),\n",
    "    'feature_fraction' : hp.uniform('feature_fraction', 0.5, 1),\n",
    "    'feature_fraction_bynode' : hp.uniform('feature_fraction_bynode', 0, 1),\n",
    "    'min_data_in_leaf' : 1 + hp.randint('min_data_in_leaf', 100),\n",
    "    'lambda_l2' : hp.uniform('lambda_l2', 0, 10),\n",
    "    'lambda_l1' : hp.uniform('lambda_l1', 0, 10),\n",
    "    'learning_rate' : hp.uniform('learning_rate', 0, 10),\n",
    "    'num_leaves' : hp.randint('num_leaves', 50, 1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_algorithm = tpe.suggest\n",
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100%|██████████| 100/100 [27:40<00:00, 16.61s/trial, best loss: 70.87096655032013]\n"
    }
   ],
   "source": [
    "best = fmin(fn = objective_2, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold Out Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the best model selected by HyperOpt is applied to a Holdout Set, which occurs right after the data used in CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.125, shuffle=False)\n",
    "X_holdout = transform_data(X_holdout)\n",
    "dhold = lgb.Dataset(X_holdout,label=y_holdout['Production'],free_raw_data=False).construct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)\n",
    "\n",
    "X_train = transform_data(X_train)\n",
    "X_val = transform_data(X_val)\n",
    "\n",
    "dtrain = lgb.Dataset(X_train,label=y_train['Production'])\n",
    "dval = lgb.Dataset(X_val,label=y_val['Production'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "For early stopping, at least one dataset and eval metric is required for evaluation",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-81df42fb9d00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprogress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    262\u001b[0m                                         \u001b[0mbegin_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_iteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                                         \u001b[0mend_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m                                         evaluation_result_list=evaluation_result_list))\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\u001b[0m in \u001b[0;36m_callback\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcmp_op\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0m_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menabled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             raise ValueError('For early stopping, '\n\u001b[0m\u001b[0;32m    192\u001b[0m                              'at least one dataset and eval metric is required for evaluation')\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: For early stopping, at least one dataset and eval metric is required for evaluation"
     ]
    }
   ],
   "source": [
    "progress = dict()\n",
    "bst = lgb.train(best, dtrain, num_boost_round=num_boost_round, valid_sets=[dval], valid_names=['eval'], early_stopping_rounds=early_stopping_rounds,verbose_eval=False,evals_result=progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'bst' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-17f0649222be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#preds = np.clip(preds,0,None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric_cnr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bst' is not defined"
     ]
    }
   ],
   "source": [
    "preds = bst.predict(dhold.get_data(),num_iteration=bst.best_iteration)\n",
    "#preds = np.clip(preds,0,None)\n",
    "score = metric_cnr(preds,dhold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Score for the HoldOut Set is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4405d376f933>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9d8a1374f0d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlen_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdhold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'True Values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r--'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Predicts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "len_x = np.arange(len(dhold.get_label()))\n",
    "plt.plot(len_x,dhold.get_label(),label='True Values')\n",
    "plt.plot(len_x,preds,'r--',label='Predicts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the best model selected is applied to all Wind Farms separately, using all Training Data for Training and Predicting all the Test Data at once (At least for the moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'bagging_fraction': 0.6162117210484838,\n 'feature_fraction': 0.8121297725774304,\n 'feature_fraction_bynode': 0.600722729840607,\n 'lambda_l1': 1.8162137175711186,\n 'lambda_l2': 2.303994028222712,\n 'learning_rate': 0.10474710600194667,\n 'max_depth': 5,\n 'min_data_in_leaf': 12,\n 'num_leaves': 771,\n 'objective': 'mean_absolute_percentage_error',\n 'metric': 'mean_absolute_percentage_error'}"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for WF in full_data['WF'].unique():\n",
    "    X_WF = full_data[full_data['WF']==WF]\n",
    "    X_train = X_WF[X_WF['Set']=='Train']\n",
    "    y_train = full_label[full_label['ID'].isin(X_train['ID'])]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)\n",
    "\n",
    "    X_test = X_WF[X_WF['Set']=='Test']\n",
    "\n",
    "    #Transform Data\n",
    "    X_train = transform_data(X_train.drop(['ID','WF','Set'],axis=1))\n",
    "    X_val = transform_data(X_val.drop(['ID','WF','Set'],axis=1))\n",
    "    X_test = transform_data(X_test.drop(['ID','WF','Set'],axis=1))\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train,label=y_train['Production'])\n",
    "    dval = lgb.Dataset(X_val,label=y_val['Production'])\n",
    "\n",
    "    bst = lgb.train(params=best,train_set=dtrain,num_boost_round=num_boost_round, valid_sets=[dval], valid_names=['eval'], early_stopping_rounds=early_stopping_rounds,verbose_eval=False)\n",
    "    pred = bst.predict(X_test)\n",
    "\n",
    "    preds = np.append(preds,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_id = pd.read_csv(r'C:\\Users\\andre_\\OneDrive\\Documentos\\GitHub\\cnr\\Data\\random_submission_example.csv')['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['ID'] = preds_id\n",
    "submission['Production'] = np.clip(preds,0,None)\n",
    "submission = submission.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Production\nID               \n37376    0.116467\n37377    0.116467\n37378    0.113701\n37379    0.114883\n37380    0.115353\n...           ...\n73900    0.621626\n73901    0.501482\n73902    0.286979\n73903    0.402168\n73904    0.346089\n\n[36529 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Production</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37376</th>\n      <td>0.116467</td>\n    </tr>\n    <tr>\n      <th>37377</th>\n      <td>0.116467</td>\n    </tr>\n    <tr>\n      <th>37378</th>\n      <td>0.113701</td>\n    </tr>\n    <tr>\n      <th>37379</th>\n      <td>0.114883</td>\n    </tr>\n    <tr>\n      <th>37380</th>\n      <td>0.115353</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>73900</th>\n      <td>0.621626</td>\n    </tr>\n    <tr>\n      <th>73901</th>\n      <td>0.501482</td>\n    </tr>\n    <tr>\n      <th>73902</th>\n      <td>0.286979</td>\n    </tr>\n    <tr>\n      <th>73903</th>\n      <td>0.402168</td>\n    </tr>\n    <tr>\n      <th>73904</th>\n      <td>0.346089</td>\n    </tr>\n  </tbody>\n</table>\n<p>36529 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(r'C:\\Users\\andre_\\OneDrive\\Documentos\\GitHub\\cnr\\Data/Submission_LightGBM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}