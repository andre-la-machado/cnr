{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginnering Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the Feature Engineering process, defined on Part 1, is applied to all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tsfresh\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from cnr_methods import get_simplified_data, transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all the process created on Part 1 is organized on Functions before applying to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features(n_features):\n",
    "    selected_features = pd.read_csv(r'Feature Selection\\Importance_WF1.csv')\n",
    "    selected_features = selected_features[:n_features]['feature']\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manual_features(feature_data):\n",
    "\n",
    "    index = feature_data.index\n",
    "    features = ['T', 'CLCT', 'U_100m','V_100m','U_10m','V_10m']\n",
    "\n",
    "    # Wind Speed Vector\n",
    "    feature_data['Wind Speed 100m'] = np.sqrt(feature_data['U_100m']**2 + feature_data['V_100m']**2)\n",
    "    feature_data['Wind Direction 100m'] = np.arctan(feature_data['V_100m']/feature_data['U_100m'])\n",
    "    feature_data['Wind Speed 10m'] = np.sqrt(feature_data['U_10m']**2 + feature_data['V_10m']**2)\n",
    "    feature_data['Wind Direction 10m'] = np.arctan(feature_data['V_10m']/feature_data['U_10m'])\n",
    "\n",
    "    feature_data['Wind Direction 100m'] = feature_data['Wind Direction 100m'].apply(lambda x: 360 + x if x < 0 else x)\n",
    "    feature_data['Wind Direction 10m'] = feature_data['Wind Direction 10m'].apply(lambda x: 360 + x if x < 0 else x)\n",
    "\n",
    "    # Time Relative Variables \n",
    "\n",
    "    for column in features:\n",
    "        feature_data[column + '_last_week'] = feature_data[column].shift(7) # Values for Last Week\n",
    "        feature_data[column + '_last_month'] = feature_data[column].shift(30) # Values for Last Month\n",
    "\n",
    "    feature_data['Month_Number'] = feature_data.index.month # Month Number\n",
    "    feature_data['Quarter_Number'] = feature_data.index.quarter # Quarter Number\n",
    "\n",
    "    mean_month = feature_data.groupby('Month_Number').mean()[features] # Month Mean\n",
    "    median_month = feature_data.groupby('Month_Number').median()[features] # Month Median\n",
    "    variance_month = feature_data.groupby('Month_Number').var()[features] # Month Variance\n",
    "\n",
    "    mean_quarter = feature_data.groupby('Quarter_Number').mean()[features] # Quarter Mean\n",
    "    median_quarter = feature_data.groupby('Quarter_Number').median()[features] # Quarter Median\n",
    "    variance_quarter = feature_data.groupby('Quarter_Number').var()[features] # Quarter Variance\n",
    "\n",
    "    mean_month.columns = mean_month.columns + '_Month_Mean'\n",
    "    median_month.columns = median_month.columns + '_Month_Median'\n",
    "    variance_month.columns = variance_month.columns + '_Month_Variance'\n",
    "    mean_quarter.columns = mean_quarter.columns + '_Quarter_Mean'\n",
    "    median_quarter.columns = median_quarter.columns + '_Quarterh_Median'\n",
    "    variance_quarter.columns = variance_quarter.columns + '_Quarter_Variance'\n",
    "\n",
    "    feature_data = feature_data.merge(mean_month,on='Month_Number',how='left')\n",
    "    feature_data = feature_data.merge(median_month,on='Month_Number',how='left')\n",
    "    feature_data = feature_data.merge(variance_month,on='Month_Number',how='left')\n",
    "    feature_data = feature_data.merge(mean_quarter,on='Quarter_Number',how='left')\n",
    "    feature_data = feature_data.merge(median_quarter,on='Quarter_Number',how='left')\n",
    "    feature_data = feature_data.merge(variance_quarter,on='Quarter_Number',how='left')\n",
    "    feature_data.index = index\n",
    "\n",
    "    # Periodical Features\n",
    "\n",
    "    day = feature_data.index.day\n",
    "    hour = feature_data.index.hour\n",
    "    minute = feature_data.index.minute\n",
    "    dayofweek = feature_data.index.dayofweek\n",
    "    dayofyear = feature_data.index.dayofyear\n",
    "    days_in_month = feature_data.index.days_in_month\n",
    "\n",
    "    feature_data[\"cos_day\"], feature_data[\"sin_day\"] = (\n",
    "    np.cos(2 * np.pi * (day - 1) / days_in_month),\n",
    "    np.sin(2 * np.pi * (day - 1) / days_in_month),\n",
    "    )\n",
    "\n",
    "    feature_data[\"cos_hour\"], feature_data[\"sin_hour\"] = (\n",
    "        np.cos(2 * np.pi * hour / 24),\n",
    "        np.sin(2 * np.pi * hour / 24),\n",
    "        )\n",
    "\n",
    "    feature_data[\"cos_minute\"], feature_data[\"sin_minute\"] = (\n",
    "        np.cos(2 * np.pi * minute / 60),\n",
    "        np.sin(2 * np.pi * minute / 60),\n",
    "    )\n",
    "\n",
    "    feature_data[\"cos_dayofyear\"], feature_data[\"sin_dayofyear\"] = (\n",
    "        np.cos(2 * np.pi * (dayofyear - 1) / 365),\n",
    "        np.sin(2 * np.pi * (dayofyear - 1) / 365),\n",
    "    )\n",
    "\n",
    "    feature_data[\"cos_dayofweek\"], feature_data[\"sin_dayofweek\"] = (\n",
    "        np.cos(2 * np.pi * dayofweek / 7),\n",
    "        np.sin(2 * np.pi * dayofweek / 7),\n",
    "    )\n",
    "\n",
    "    # Distance from Max and Min\n",
    "\n",
    "    for column in features:\n",
    "        feature_data[column + '_Distance_Max'] = feature_data.index - feature_data[column].idxmax()\n",
    "        feature_data[column + '_Distance_Min'] = feature_data.index - feature_data[column].idxmin()\n",
    "        feature_data[column + '_Distance_Max'] = feature_data[column + '_Distance_Max'].apply(lambda x : x.days)\n",
    "        feature_data[column + '_Distance_Min'] = feature_data[column + '_Distance_Min'].apply(lambda x : x.days)\n",
    "\n",
    "    # Dropping Base Features \n",
    "    #features.append(['Month_Number','Quarter Number'])\n",
    "    feature_data = feature_data.drop(features,axis=1)\n",
    "\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsfresh_features(data):\n",
    "    tsfresh_data = pd.DataFrame()\n",
    "    for variable in ['U_100m','V_100m','U_10m','V_10m','T','CLCT']: \n",
    "        df_shift, y = make_forecasting_frame(data[variable],kind=variable,max_timeshift=20,rolling_direction=1)\n",
    "        X = extract_features(df_shift, column_id=\"id\", column_sort=\"time\", column_value=\"value\", impute_function=impute,show_warnings=False,n_jobs=3)\n",
    "        X['Feature'] = variable\n",
    "        tsfresh_data = tsfresh_data.append(X)\n",
    "\n",
    "    # Formmating the Data\n",
    "    tsfresh_data = tsfresh_data.pivot(columns='Feature')\n",
    "    tsfresh_data.columns = tsfresh_data.columns.map('{0[0]}|{0[1]}'.format)\n",
    "    #tsfresh_data = tsfresh_data.loc[:, tsfresh_data.apply(pd.Series.nunique) != 1] # Remove Constant Columns\n",
    "\n",
    "    return tsfresh_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_data(X): \n",
    "    feature_data = pd.DataFrame()\n",
    "    for WF in X['WF'].unique():\n",
    "        X_WF = X[X['WF']==WF]\n",
    "\n",
    "        X_manual = get_manual_features(X_WF)\n",
    "        X_tsfresh = get_tsfresh_features(X_WF)\n",
    "        X_WF = pd.concat([X_manual,X_tsfresh],axis=1)\n",
    "        X_WF['WF'] = WF\n",
    "        feature_data = pd.concat([feature_data,X_WF],axis=0)\n",
    "\n",
    "    feature_data = pd.concat([X,feature_data],axis=1) \n",
    "\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the Feature Engineering is properly applied on the Full Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y_train = get_simplified_data()\n",
    "\n",
    "X_train = X[X['Set']=='Train']\n",
    "X_test = X[X['Set']=='Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 15/15 [02:30<00:00, 10.00s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:23<00:00,  9.58s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:21<00:00,  9.47s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:19<00:00,  9.32s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.10s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:45<00:00,  7.02s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.10s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.01s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.11s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.08s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:13<00:00,  8.88s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.16s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.99s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:17<00:00,  9.17s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:17<00:00,  9.15s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.13s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:11<00:00,  8.75s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.20s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.05s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.05s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.06s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:17<00:00,  9.14s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:11<00:00,  8.80s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.17s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.99s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.03s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.02s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:10<00:00,  8.71s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.19s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:18<00:00,  9.25s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.07s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.12s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.09s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:12<00:00,  8.80s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:46<00:00,  7.13s/it]\n"
    }
   ],
   "source": [
    "X_train = get_features_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.95s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:13<00:00,  8.93s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.08s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:08<00:00,  8.54s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:39<00:00,  6.61s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.97s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.06s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.97s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.01s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:10<00:00,  8.67s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:42<00:00,  6.85s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.04s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:10<00:00,  8.67s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:42<00:00,  6.84s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.94s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.95s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.02s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.95s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:08<00:00,  8.55s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:44<00:00,  6.94s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:00<00:00,  8.06s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:02<00:00,  8.13s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:01<00:00,  8.10s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:03<00:00,  8.27s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:56<00:00,  7.78s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:34<00:00,  6.28s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.99s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:20<00:00,  9.39s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:05<00:00,  8.40s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:05<00:00,  8.38s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:03<00:00,  8.23s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:33<00:00,  6.22s/it]\n"
    }
   ],
   "source": [
    "X_test = get_features_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Set'] = 'Train'\n",
    "X_test['Set'] = 'Test'\n",
    "\n",
    "feature_data = pd.concat([X_train,X_test],axis=0)\n",
    "\n",
    "feature_data = feature_data.loc[:,~feature_data.columns.duplicated()]\n",
    "\n",
    "\n",
    "feature_data.to_csv('Feature Selection/Selected_Features_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                        ID   WF    U_100m    V_100m     U_10m     V_10m  \\\n2018-05-01 01:00:00      1  WF1 -2.248500 -3.257800  1.254603 -0.289687   \n2018-05-01 02:00:00      2  WF1 -2.434500 -1.446100  2.490908 -0.413370   \n2018-05-01 03:00:00      3  WF1 -1.707402 -0.853745  0.997093 -1.415138   \n2018-05-01 04:00:00      4  WF1  3.706500 -6.217400  0.689598 -0.961441   \n2018-05-01 05:00:00      5  WF1  3.813400 -5.444600  0.290994 -0.294963   \n...                    ...  ...       ...       ...       ...       ...   \n2019-09-30 18:00:00  73900  WF6 -3.039992 -4.250952 -0.530133 -1.421466   \n2019-09-30 19:00:00  73901  WF6 -3.204700 -4.348800  0.121283 -0.885161   \n2019-09-30 20:00:00  73902  WF6 -0.746600 -4.334300  0.080257 -0.690724   \n2019-09-30 21:00:00  73903  WF6 -0.169990 -2.814300 -0.057787 -0.809309   \n2019-09-30 22:00:00  73904  WF6  1.647200 -3.907000 -0.460279 -0.790079   \n\n                              T       CLCT    Set  Wind Speed 100m  ...  \\\n2018-05-01 01:00:00  286.440000  82.543144  Train         3.958410  ...   \n2018-05-01 02:00:00  286.260000  99.990844  Train         2.831607  ...   \n2018-05-01 03:00:00  287.000000  98.367235  Train         1.908954  ...   \n2018-05-01 04:00:00  284.780000  94.860604  Train         7.238384  ...   \n2018-05-01 05:00:00  284.460000  95.905879  Train         6.647232  ...   \n...                         ...        ...    ...              ...  ...   \n2019-09-30 18:00:00  292.389997   0.000000   Test         5.226102  ...   \n2019-09-30 19:00:00  292.480000   0.000000   Test         5.402052  ...   \n2019-09-30 20:00:00  291.340000   0.000000   Test         4.398132  ...   \n2019-09-30 21:00:00  289.635009   0.000000   Test         2.819429  ...   \n2019-09-30 22:00:00  289.880000   0.000000   Test         4.240037  ...   \n\n                     value__variance|U_100m  value__variance|U_10m  \\\n2018-05-01 01:00:00                     NaN                    NaN   \n2018-05-01 02:00:00                0.000000               0.000000   \n2018-05-01 03:00:00                0.008649               0.382112   \n2018-05-01 04:00:00                0.095117               0.425138   \n2018-05-01 05:00:00                6.458769               0.467797   \n...                                     ...                    ...   \n2019-09-30 18:00:00                1.571467               0.519446   \n2019-09-30 19:00:00                1.917278               0.519429   \n2019-09-30 20:00:00                2.271335               0.535901   \n2019-09-30 21:00:00                2.123036               0.548414   \n2019-09-30 22:00:00                2.105098               0.545908   \n\n                     value__variance|V_100m  value__variance|V_10m  \\\n2018-05-01 01:00:00                     NaN                    NaN   \n2018-05-01 02:00:00                0.000000               0.000000   \n2018-05-01 03:00:00                0.820564               0.003824   \n2018-05-01 04:00:00                1.045847               0.253942   \n2018-05-01 05:00:00                4.356622               0.202684   \n...                                     ...                    ...   \n2019-09-30 18:00:00               19.502184               6.778447   \n2019-09-30 19:00:00               13.911188               4.784000   \n2019-09-30 20:00:00                8.168720               3.101331   \n2019-09-30 21:00:00                4.847935               2.601988   \n2019-09-30 22:00:00                2.824064               2.301282   \n\n                     value__variance_larger_than_standard_deviation|CLCT  \\\n2018-05-01 01:00:00                                                NaN     \n2018-05-01 02:00:00                                                0.0     \n2018-05-01 03:00:00                                                1.0     \n2018-05-01 04:00:00                                                1.0     \n2018-05-01 05:00:00                                                1.0     \n...                                                                ...     \n2019-09-30 18:00:00                                                1.0     \n2019-09-30 19:00:00                                                1.0     \n2019-09-30 20:00:00                                                1.0     \n2019-09-30 21:00:00                                                1.0     \n2019-09-30 22:00:00                                                1.0     \n\n                     value__variance_larger_than_standard_deviation|T  \\\n2018-05-01 01:00:00                                               NaN   \n2018-05-01 02:00:00                                               0.0   \n2018-05-01 03:00:00                                               0.0   \n2018-05-01 04:00:00                                               0.0   \n2018-05-01 05:00:00                                               0.0   \n...                                                               ...   \n2019-09-30 18:00:00                                               1.0   \n2019-09-30 19:00:00                                               1.0   \n2019-09-30 20:00:00                                               1.0   \n2019-09-30 21:00:00                                               1.0   \n2019-09-30 22:00:00                                               1.0   \n\n                     value__variance_larger_than_standard_deviation|U_100m  \\\n2018-05-01 01:00:00                                                NaN       \n2018-05-01 02:00:00                                                0.0       \n2018-05-01 03:00:00                                                0.0       \n2018-05-01 04:00:00                                                0.0       \n2018-05-01 05:00:00                                                1.0       \n...                                                                ...       \n2019-09-30 18:00:00                                                1.0       \n2019-09-30 19:00:00                                                1.0       \n2019-09-30 20:00:00                                                1.0       \n2019-09-30 21:00:00                                                1.0       \n2019-09-30 22:00:00                                                1.0       \n\n                     value__variance_larger_than_standard_deviation|U_10m  \\\n2018-05-01 01:00:00                                                NaN      \n2018-05-01 02:00:00                                                0.0      \n2018-05-01 03:00:00                                                0.0      \n2018-05-01 04:00:00                                                0.0      \n2018-05-01 05:00:00                                                0.0      \n...                                                                ...      \n2019-09-30 18:00:00                                                0.0      \n2019-09-30 19:00:00                                                0.0      \n2019-09-30 20:00:00                                                0.0      \n2019-09-30 21:00:00                                                0.0      \n2019-09-30 22:00:00                                                0.0      \n\n                     value__variance_larger_than_standard_deviation|V_100m  \\\n2018-05-01 01:00:00                                                NaN       \n2018-05-01 02:00:00                                                0.0       \n2018-05-01 03:00:00                                                0.0       \n2018-05-01 04:00:00                                                1.0       \n2018-05-01 05:00:00                                                1.0       \n...                                                                ...       \n2019-09-30 18:00:00                                                1.0       \n2019-09-30 19:00:00                                                1.0       \n2019-09-30 20:00:00                                                1.0       \n2019-09-30 21:00:00                                                1.0       \n2019-09-30 22:00:00                                                1.0       \n\n                     value__variance_larger_than_standard_deviation|V_10m  \n2018-05-01 01:00:00                                                NaN     \n2018-05-01 02:00:00                                                0.0     \n2018-05-01 03:00:00                                                0.0     \n2018-05-01 04:00:00                                                0.0     \n2018-05-01 05:00:00                                                0.0     \n...                                                                ...     \n2019-09-30 18:00:00                                                1.0     \n2019-09-30 19:00:00                                                1.0     \n2019-09-30 20:00:00                                                1.0     \n2019-09-30 21:00:00                                                1.0     \n2019-09-30 22:00:00                                                1.0     \n\n[73904 rows x 4621 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>WF</th>\n      <th>U_100m</th>\n      <th>V_100m</th>\n      <th>U_10m</th>\n      <th>V_10m</th>\n      <th>T</th>\n      <th>CLCT</th>\n      <th>Set</th>\n      <th>Wind Speed 100m</th>\n      <th>...</th>\n      <th>value__variance|U_100m</th>\n      <th>value__variance|U_10m</th>\n      <th>value__variance|V_100m</th>\n      <th>value__variance|V_10m</th>\n      <th>value__variance_larger_than_standard_deviation|CLCT</th>\n      <th>value__variance_larger_than_standard_deviation|T</th>\n      <th>value__variance_larger_than_standard_deviation|U_100m</th>\n      <th>value__variance_larger_than_standard_deviation|U_10m</th>\n      <th>value__variance_larger_than_standard_deviation|V_100m</th>\n      <th>value__variance_larger_than_standard_deviation|V_10m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-05-01 01:00:00</th>\n      <td>1</td>\n      <td>WF1</td>\n      <td>-2.248500</td>\n      <td>-3.257800</td>\n      <td>1.254603</td>\n      <td>-0.289687</td>\n      <td>286.440000</td>\n      <td>82.543144</td>\n      <td>Train</td>\n      <td>3.958410</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 02:00:00</th>\n      <td>2</td>\n      <td>WF1</td>\n      <td>-2.434500</td>\n      <td>-1.446100</td>\n      <td>2.490908</td>\n      <td>-0.413370</td>\n      <td>286.260000</td>\n      <td>99.990844</td>\n      <td>Train</td>\n      <td>2.831607</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 03:00:00</th>\n      <td>3</td>\n      <td>WF1</td>\n      <td>-1.707402</td>\n      <td>-0.853745</td>\n      <td>0.997093</td>\n      <td>-1.415138</td>\n      <td>287.000000</td>\n      <td>98.367235</td>\n      <td>Train</td>\n      <td>1.908954</td>\n      <td>...</td>\n      <td>0.008649</td>\n      <td>0.382112</td>\n      <td>0.820564</td>\n      <td>0.003824</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 04:00:00</th>\n      <td>4</td>\n      <td>WF1</td>\n      <td>3.706500</td>\n      <td>-6.217400</td>\n      <td>0.689598</td>\n      <td>-0.961441</td>\n      <td>284.780000</td>\n      <td>94.860604</td>\n      <td>Train</td>\n      <td>7.238384</td>\n      <td>...</td>\n      <td>0.095117</td>\n      <td>0.425138</td>\n      <td>1.045847</td>\n      <td>0.253942</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2018-05-01 05:00:00</th>\n      <td>5</td>\n      <td>WF1</td>\n      <td>3.813400</td>\n      <td>-5.444600</td>\n      <td>0.290994</td>\n      <td>-0.294963</td>\n      <td>284.460000</td>\n      <td>95.905879</td>\n      <td>Train</td>\n      <td>6.647232</td>\n      <td>...</td>\n      <td>6.458769</td>\n      <td>0.467797</td>\n      <td>4.356622</td>\n      <td>0.202684</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2019-09-30 18:00:00</th>\n      <td>73900</td>\n      <td>WF6</td>\n      <td>-3.039992</td>\n      <td>-4.250952</td>\n      <td>-0.530133</td>\n      <td>-1.421466</td>\n      <td>292.389997</td>\n      <td>0.000000</td>\n      <td>Test</td>\n      <td>5.226102</td>\n      <td>...</td>\n      <td>1.571467</td>\n      <td>0.519446</td>\n      <td>19.502184</td>\n      <td>6.778447</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2019-09-30 19:00:00</th>\n      <td>73901</td>\n      <td>WF6</td>\n      <td>-3.204700</td>\n      <td>-4.348800</td>\n      <td>0.121283</td>\n      <td>-0.885161</td>\n      <td>292.480000</td>\n      <td>0.000000</td>\n      <td>Test</td>\n      <td>5.402052</td>\n      <td>...</td>\n      <td>1.917278</td>\n      <td>0.519429</td>\n      <td>13.911188</td>\n      <td>4.784000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2019-09-30 20:00:00</th>\n      <td>73902</td>\n      <td>WF6</td>\n      <td>-0.746600</td>\n      <td>-4.334300</td>\n      <td>0.080257</td>\n      <td>-0.690724</td>\n      <td>291.340000</td>\n      <td>0.000000</td>\n      <td>Test</td>\n      <td>4.398132</td>\n      <td>...</td>\n      <td>2.271335</td>\n      <td>0.535901</td>\n      <td>8.168720</td>\n      <td>3.101331</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2019-09-30 21:00:00</th>\n      <td>73903</td>\n      <td>WF6</td>\n      <td>-0.169990</td>\n      <td>-2.814300</td>\n      <td>-0.057787</td>\n      <td>-0.809309</td>\n      <td>289.635009</td>\n      <td>0.000000</td>\n      <td>Test</td>\n      <td>2.819429</td>\n      <td>...</td>\n      <td>2.123036</td>\n      <td>0.548414</td>\n      <td>4.847935</td>\n      <td>2.601988</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2019-09-30 22:00:00</th>\n      <td>73904</td>\n      <td>WF6</td>\n      <td>1.647200</td>\n      <td>-3.907000</td>\n      <td>-0.460279</td>\n      <td>-0.790079</td>\n      <td>289.880000</td>\n      <td>0.000000</td>\n      <td>Test</td>\n      <td>4.240037</td>\n      <td>...</td>\n      <td>2.105098</td>\n      <td>0.545908</td>\n      <td>2.824064</td>\n      <td>2.301282</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>73904 rows × 4621 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}