{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginnering Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the Feature Engineering process, defined on Part 1, is applied to all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tsfresh\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import make_forecasting_frame\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from cnr_methods import get_simplified_data, transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all the process created on Part 1 is organized on Functions before applying to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_features(n_features):\n",
    "    selected_features = pd.read_csv(r'Feature Selection\\Importance_WF1.csv')\n",
    "    selected_features = selected_features[:n_features]['feature']\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manual_features(feature_data):\n",
    "\n",
    "    index = feature_data.index\n",
    "    features = ['T', 'CLCT', 'U_100m','V_100m','U_10m','V_10m']\n",
    "\n",
    "    # Wind Speed Vector\n",
    "    feature_data['Wind Speed 100m'] = np.sqrt(feature_data['U_100m']**2 + feature_data['V_100m']**2)\n",
    "    feature_data['Wind Direction 100m'] = np.arctan(feature_data['V_100m']/feature_data['U_100m'])\n",
    "    feature_data['Wind Speed 10m'] = np.sqrt(feature_data['U_10m']**2 + feature_data['V_10m']**2)\n",
    "    feature_data['Wind Direction 10m'] = np.arctan(feature_data['V_10m']/feature_data['U_10m'])\n",
    "\n",
    "    feature_data['Wind Direction 100m'] = feature_data['Wind Direction 100m'].apply(lambda x: 360 + x if x < 0 else x)\n",
    "    feature_data['Wind Direction 10m'] = feature_data['Wind Direction 10m'].apply(lambda x: 360 + x if x < 0 else x)\n",
    "\n",
    "    # Time Relative Variables \n",
    "\n",
    "    for column in features:\n",
    "        feature_data[column + '_last_week'] = feature_data[column].shift(7) # Values for Last Week\n",
    "        feature_data[column + '_last_month'] = feature_data[column].shift(30) # Values for Last Month\n",
    "\n",
    "    feature_data['Month_Number'] = feature_data.index.month # Month Number\n",
    "    feature_data['Quarter_Number'] = feature_data.index.quarter # Quarter Number\n",
    "\n",
    "    mean_month = feature_data.groupby('Month_Number').mean()[features] # Month Mean\n",
    "    median_month = feature_data.groupby('Month_Number').median()[features] # Month Median\n",
    "    variance_month = feature_data.groupby('Month_Number').var()[features] # Month Variance\n",
    "\n",
    "    mean_quarter = feature_data.groupby('Quarter_Number').mean()[features] # Quarter Mean\n",
    "    median_quarter = feature_data.groupby('Quarter_Number').median()[features] # Quarter Median\n",
    "    variance_quarter = feature_data.groupby('Quarter_Number').var()[features] # Quarter Variance\n",
    "\n",
    "    mean_month.columns = mean_month.columns + '_Month_Mean'\n",
    "    median_month.columns = median_month.columns + '_Month_Median'\n",
    "    variance_month.columns = variance_month.columns + '_Month_Variance'\n",
    "    mean_quarter.columns = mean_quarter.columns + '_Quarter_Mean'\n",
    "    median_quarter.columns = median_quarter.columns + '_Quarterh_Median'\n",
    "    variance_quarter.columns = variance_quarter.columns + '_Quarter_Variance'\n",
    "\n",
    "    feature_data = feature_data.merge(mean_month,on='Month_Number',how='left')\n",
    "    feature_data = feature_data.merge(median_month,on='Month_Number',how='left')\n",
    "    feature_data = feature_data.merge(variance_month,on='Month_Number',how='left')\n",
    "    feature_data = feature_data.merge(mean_quarter,on='Quarter_Number',how='left')\n",
    "    feature_data = feature_data.merge(median_quarter,on='Quarter_Number',how='left')\n",
    "    feature_data = feature_data.merge(variance_quarter,on='Quarter_Number',how='left')\n",
    "    feature_data.index = index\n",
    "\n",
    "    # Periodical Features\n",
    "\n",
    "    day = feature_data.index.day\n",
    "    hour = feature_data.index.hour\n",
    "    minute = feature_data.index.minute\n",
    "    dayofweek = feature_data.index.dayofweek\n",
    "    dayofyear = feature_data.index.dayofyear\n",
    "    days_in_month = feature_data.index.days_in_month\n",
    "\n",
    "    feature_data[\"cos_day\"], feature_data[\"sin_day\"] = (\n",
    "    np.cos(2 * np.pi * (day - 1) / days_in_month),\n",
    "    np.sin(2 * np.pi * (day - 1) / days_in_month),\n",
    "    )\n",
    "\n",
    "    feature_data[\"cos_hour\"], feature_data[\"sin_hour\"] = (\n",
    "        np.cos(2 * np.pi * hour / 24),\n",
    "        np.sin(2 * np.pi * hour / 24),\n",
    "        )\n",
    "\n",
    "    feature_data[\"cos_minute\"], feature_data[\"sin_minute\"] = (\n",
    "        np.cos(2 * np.pi * minute / 60),\n",
    "        np.sin(2 * np.pi * minute / 60),\n",
    "    )\n",
    "\n",
    "    feature_data[\"cos_dayofyear\"], feature_data[\"sin_dayofyear\"] = (\n",
    "        np.cos(2 * np.pi * (dayofyear - 1) / 365),\n",
    "        np.sin(2 * np.pi * (dayofyear - 1) / 365),\n",
    "    )\n",
    "\n",
    "    feature_data[\"cos_dayofweek\"], feature_data[\"sin_dayofweek\"] = (\n",
    "        np.cos(2 * np.pi * dayofweek / 7),\n",
    "        np.sin(2 * np.pi * dayofweek / 7),\n",
    "    )\n",
    "\n",
    "    # Distance from Max and Min\n",
    "\n",
    "    for column in features:\n",
    "        feature_data[column + '_Distance_Max'] = feature_data.index - feature_data[column].idxmax()\n",
    "        feature_data[column + '_Distance_Min'] = feature_data.index - feature_data[column].idxmin()\n",
    "        feature_data[column + '_Distance_Max'] = feature_data[column + '_Distance_Max'].apply(lambda x : x.days)\n",
    "        feature_data[column + '_Distance_Min'] = feature_data[column + '_Distance_Min'].apply(lambda x : x.days)\n",
    "\n",
    "    # Dropping Base Features \n",
    "    #features.append(['Month_Number','Quarter Number'])\n",
    "    feature_data = feature_data.drop(features,axis=1)\n",
    "\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsfresh_features(data):\n",
    "    tsfresh_data = pd.DataFrame()\n",
    "    for variable in ['U_100m','V_100m','U_10m','V_10m','T','CLCT']: \n",
    "        df_shift, y = make_forecasting_frame(data[variable],kind=variable,max_timeshift=20,rolling_direction=1)\n",
    "        X = extract_features(df_shift, column_id=\"id\", column_sort=\"time\", column_value=\"value\", impute_function=impute,show_warnings=False,n_jobs=3)\n",
    "        X['Feature'] = variable\n",
    "        tsfresh_data = tsfresh_data.append(X)\n",
    "\n",
    "    # Formmating the Data\n",
    "    tsfresh_data = tsfresh_data.pivot(columns='Feature')\n",
    "    tsfresh_data.columns = tsfresh_data.columns.map('{0[0]}|{0[1]}'.format)\n",
    "    #tsfresh_data = tsfresh_data.loc[:, tsfresh_data.apply(pd.Series.nunique) != 1] # Remove Constant Columns\n",
    "\n",
    "    return tsfresh_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_data(X): \n",
    "    feature_data = pd.DataFrame()\n",
    "    for WF in X['WF'].unique():\n",
    "        X_WF = X[X['WF']==WF]\n",
    "\n",
    "        X_manual = get_manual_features(X_WF)\n",
    "        X_tsfresh = get_tsfresh_features(X_WF)\n",
    "        X_WF = pd.concat([X_manual,X_tsfresh],axis=1)\n",
    "        X_WF['WF'] = WF\n",
    "        feature_data = pd.concat([feature_data,X_WF],axis=0)\n",
    "\n",
    "    feature_data = pd.concat([X,feature_data],axis=1) \n",
    "\n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the Feature Engineering is properly applied on the Full Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y_train = get_simplified_data()\n",
    "\n",
    "X_train = X[X['Set']=='Train']\n",
    "X_test = X[X['Set']=='Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 15/15 [02:30<00:00, 10.00s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:23<00:00,  9.58s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:21<00:00,  9.47s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:19<00:00,  9.32s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.10s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:45<00:00,  7.02s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.10s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.01s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.11s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.08s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:13<00:00,  8.88s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.16s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.99s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:17<00:00,  9.17s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:17<00:00,  9.15s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.13s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:11<00:00,  8.75s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.20s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.05s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.05s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.06s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:17<00:00,  9.14s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:11<00:00,  8.80s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.17s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.99s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.03s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.02s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:10<00:00,  8.71s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:47<00:00,  7.19s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:18<00:00,  9.25s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.07s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.12s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.09s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:12<00:00,  8.80s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:46<00:00,  7.13s/it]\n"
    }
   ],
   "source": [
    "X_train = get_features_data(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Feature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.95s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:13<00:00,  8.93s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:16<00:00,  9.08s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:08<00:00,  8.54s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:39<00:00,  6.61s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.97s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.06s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.97s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.01s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:10<00:00,  8.67s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:42<00:00,  6.85s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.04s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.98s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:10<00:00,  8.67s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:42<00:00,  6.84s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.94s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.95s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:15<00:00,  9.02s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.95s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:08<00:00,  8.55s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:44<00:00,  6.94s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:00<00:00,  8.06s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:02<00:00,  8.13s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:01<00:00,  8.10s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:03<00:00,  8.27s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:56<00:00,  7.78s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:34<00:00,  6.28s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:14<00:00,  8.99s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:20<00:00,  9.39s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:05<00:00,  8.40s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:05<00:00,  8.38s/it]\nFeature Extraction: 100%|██████████| 15/15 [02:03<00:00,  8.23s/it]\nFeature Extraction: 100%|██████████| 15/15 [01:33<00:00,  6.22s/it]\n"
    }
   ],
   "source": [
    "X_test = get_features_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Set'] = 'Train'\n",
    "X_test['Set'] = 'Test'\n",
    "\n",
    "feature_data = pd.concat([X_train,X_test],axis=0)\n",
    "\n",
    "feature_data = feature_data.loc[:,~feature_data.columns.duplicated()]\n",
    "\n",
    "\n",
    "feature_data.to_csv('Feature Selection/Selected_Features_Data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}