{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1596670974807",
      "display_name": "Python 3.7.6 64-bit ('base': conda)"
    },
    "colab": {
      "name": "modelling_lstm_colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andre-la-machado/cnr/blob/master/modelling_lstm_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUg3rsB0MaUT",
        "colab_type": "text"
      },
      "source": [
        "# LSTM Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2sEClXHMaUU",
        "colab_type": "text"
      },
      "source": [
        "On this Notebook, a LSTM Model will be tried for the Competitition Data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgcFrkVNMaUV",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRldKXObNEeh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd5e276-ea68-4711-908a-83f27e63d72f"
      },
      "source": [
        "%pip uninstall -q -y tensorflow\n",
        "%pip install -q -U tensorflow-gpu>=2\n",
        "%reset -f"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkGRpSoARdI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "bd906410-9a65-4d39-f960-53837a34f2a9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Sep 23 00:44:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    25W /  75W |    483MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPzbEtzOMaUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "sns.set_style('darkgrid')\n",
        "from cnr_methods import get_selected_features, transform_data, revert_data, get_simplified_data\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "EJt5BAyhMaUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "df3c3f5f-a44a-42b0-cd80-ac391135e750"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.test.is_built_with_cuda())  # Sanity check: GPU available to tf or not\n",
        "print(tf.test.is_built_with_gpu_support())\n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "print(tf.__version__)  # Check if __version__>=\"2.0.0\"\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "2.3.0\n",
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfHnGOTHMaUd",
        "colab_type": "text"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJx9hbTsMaUd",
        "colab_type": "text"
      },
      "source": [
        "Here, the data used correspond to the results of the Feature Engineering and Selection Step. For simplicity, during Hyperparameter Optimization, only Wind Farm 3 Training Data is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzHSYL52MaUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data = pd.read_csv(\"Selected_Features_Data.csv\")\n",
        "\n",
        "#full_data = full_data.rename({'Unnamed: 0' : 'Time'},axis=1)\n",
        "full_data = full_data.set_index('Time')\n",
        "\n",
        "full_label = pd.read_csv('Y_train.csv')\n",
        "X = full_data[full_data['Set']=='Train']\n",
        "\n",
        "WF = 'WF3'\n",
        "X = X[X['WF']==WF]\n",
        "y = full_label[full_label['ID'].isin(X['ID'])]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3dqO0HFMaUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6e67ec4b-c421-488d-e64f-c02f9b4f6348"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>WF</th>\n",
              "      <th>U_100m</th>\n",
              "      <th>V_100m</th>\n",
              "      <th>U_10m</th>\n",
              "      <th>V_10m</th>\n",
              "      <th>T</th>\n",
              "      <th>CLCT</th>\n",
              "      <th>Set</th>\n",
              "      <th>Wind Speed 100m</th>\n",
              "      <th>Wind Direction 100m</th>\n",
              "      <th>Wind Speed 10m</th>\n",
              "      <th>Wind Direction 10m</th>\n",
              "      <th>T_lag_7_days</th>\n",
              "      <th>T_lag_14_days</th>\n",
              "      <th>T_lag_21_days</th>\n",
              "      <th>CLCT_lag_7_days</th>\n",
              "      <th>CLCT_lag_14_days</th>\n",
              "      <th>CLCT_lag_21_days</th>\n",
              "      <th>U_100m_lag_7_days</th>\n",
              "      <th>U_100m_lag_14_days</th>\n",
              "      <th>U_100m_lag_21_days</th>\n",
              "      <th>V_100m_lag_7_days</th>\n",
              "      <th>V_100m_lag_14_days</th>\n",
              "      <th>V_100m_lag_21_days</th>\n",
              "      <th>U_10m_lag_7_days</th>\n",
              "      <th>U_10m_lag_14_days</th>\n",
              "      <th>U_10m_lag_21_days</th>\n",
              "      <th>V_10m_lag_7_days</th>\n",
              "      <th>V_10m_lag_14_days</th>\n",
              "      <th>V_10m_lag_21_days</th>\n",
              "      <th>Month_Number</th>\n",
              "      <th>T_Last_Month_Mean</th>\n",
              "      <th>CLCT_Last_Month_Mean</th>\n",
              "      <th>U_100m_Last_Month_Mean</th>\n",
              "      <th>V_100m_Last_Month_Mean</th>\n",
              "      <th>U_10m_Last_Month_Mean</th>\n",
              "      <th>V_10m_Last_Month_Mean</th>\n",
              "      <th>TLast_Month_Variance</th>\n",
              "      <th>CLCTLast_Month_Variance</th>\n",
              "      <th>...</th>\n",
              "      <th>CLCT_Distance_Max</th>\n",
              "      <th>CLCT_Distance_Min</th>\n",
              "      <th>U_100m_Distance_Max</th>\n",
              "      <th>U_100m_Distance_Min</th>\n",
              "      <th>V_100m_Distance_Max</th>\n",
              "      <th>V_100m_Distance_Min</th>\n",
              "      <th>U_10m_Distance_Max</th>\n",
              "      <th>U_10m_Distance_Min</th>\n",
              "      <th>V_10m_Distance_Max</th>\n",
              "      <th>V_10m_Distance_Min</th>\n",
              "      <th>T_Rolling_7_Window_Mean</th>\n",
              "      <th>T_Rolling_14_Window_Mean</th>\n",
              "      <th>T_Rolling_7_Window_Variance</th>\n",
              "      <th>T_Rolling_14_Window_Variance</th>\n",
              "      <th>CLCT_Rolling_7_Window_Mean</th>\n",
              "      <th>CLCT_Rolling_14_Window_Mean</th>\n",
              "      <th>CLCT_Rolling_7_Window_Variance</th>\n",
              "      <th>CLCT_Rolling_14_Window_Variance</th>\n",
              "      <th>U_100m_Rolling_7_Window_Mean</th>\n",
              "      <th>U_100m_Rolling_14_Window_Mean</th>\n",
              "      <th>U_100m_Rolling_7_Window_Variance</th>\n",
              "      <th>U_100m_Rolling_14_Window_Variance</th>\n",
              "      <th>V_100m_Rolling_7_Window_Mean</th>\n",
              "      <th>V_100m_Rolling_14_Window_Mean</th>\n",
              "      <th>V_100m_Rolling_7_Window_Variance</th>\n",
              "      <th>V_100m_Rolling_14_Window_Variance</th>\n",
              "      <th>U_10m_Rolling_7_Window_Mean</th>\n",
              "      <th>U_10m_Rolling_14_Window_Mean</th>\n",
              "      <th>U_10m_Rolling_7_Window_Variance</th>\n",
              "      <th>U_10m_Rolling_14_Window_Variance</th>\n",
              "      <th>V_10m_Rolling_7_Window_Mean</th>\n",
              "      <th>V_10m_Rolling_14_Window_Mean</th>\n",
              "      <th>V_10m_Rolling_7_Window_Variance</th>\n",
              "      <th>V_10m_Rolling_14_Window_Variance</th>\n",
              "      <th>T_Expanded_Window_Max</th>\n",
              "      <th>CLCT_Expanded_Window_Max</th>\n",
              "      <th>U_100m_Expanded_Window_Max</th>\n",
              "      <th>V_100m_Expanded_Window_Max</th>\n",
              "      <th>U_10m_Expanded_Window_Max</th>\n",
              "      <th>V_10m_Expanded_Window_Max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-05-01 01:00:00</th>\n",
              "      <td>12479</td>\n",
              "      <td>WF3</td>\n",
              "      <td>5.789500</td>\n",
              "      <td>3.820200</td>\n",
              "      <td>1.054669</td>\n",
              "      <td>1.317597</td>\n",
              "      <td>275.690</td>\n",
              "      <td>86.504507</td>\n",
              "      <td>Train</td>\n",
              "      <td>6.936299</td>\n",
              "      <td>0.583268</td>\n",
              "      <td>1.687717</td>\n",
              "      <td>0.895782</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-223</td>\n",
              "      <td>-201</td>\n",
              "      <td>-193</td>\n",
              "      <td>-160</td>\n",
              "      <td>-223</td>\n",
              "      <td>-227</td>\n",
              "      <td>-193</td>\n",
              "      <td>-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>275.690</td>\n",
              "      <td>86.504507</td>\n",
              "      <td>5.7895</td>\n",
              "      <td>3.8202</td>\n",
              "      <td>1.054669</td>\n",
              "      <td>1.317597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01 02:00:00</th>\n",
              "      <td>12480</td>\n",
              "      <td>WF3</td>\n",
              "      <td>6.003300</td>\n",
              "      <td>3.920600</td>\n",
              "      <td>0.876879</td>\n",
              "      <td>1.483483</td>\n",
              "      <td>275.770</td>\n",
              "      <td>98.976088</td>\n",
              "      <td>Train</td>\n",
              "      <td>7.170127</td>\n",
              "      <td>0.578533</td>\n",
              "      <td>1.723264</td>\n",
              "      <td>1.036951</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-223</td>\n",
              "      <td>-201</td>\n",
              "      <td>-193</td>\n",
              "      <td>-160</td>\n",
              "      <td>-223</td>\n",
              "      <td>-227</td>\n",
              "      <td>-193</td>\n",
              "      <td>-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>275.770</td>\n",
              "      <td>98.976088</td>\n",
              "      <td>6.0033</td>\n",
              "      <td>3.9206</td>\n",
              "      <td>1.054669</td>\n",
              "      <td>1.483483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01 03:00:00</th>\n",
              "      <td>12481</td>\n",
              "      <td>WF3</td>\n",
              "      <td>5.931829</td>\n",
              "      <td>0.907656</td>\n",
              "      <td>0.949640</td>\n",
              "      <td>1.419591</td>\n",
              "      <td>276.875</td>\n",
              "      <td>64.193607</td>\n",
              "      <td>Train</td>\n",
              "      <td>6.000870</td>\n",
              "      <td>0.151837</td>\n",
              "      <td>1.707938</td>\n",
              "      <td>0.981212</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-223</td>\n",
              "      <td>-201</td>\n",
              "      <td>-193</td>\n",
              "      <td>-160</td>\n",
              "      <td>-223</td>\n",
              "      <td>-227</td>\n",
              "      <td>-193</td>\n",
              "      <td>-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276.875</td>\n",
              "      <td>98.976088</td>\n",
              "      <td>6.0033</td>\n",
              "      <td>3.9206</td>\n",
              "      <td>1.054669</td>\n",
              "      <td>1.483483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01 04:00:00</th>\n",
              "      <td>12482</td>\n",
              "      <td>WF3</td>\n",
              "      <td>5.205300</td>\n",
              "      <td>1.683800</td>\n",
              "      <td>1.027462</td>\n",
              "      <td>1.029786</td>\n",
              "      <td>275.650</td>\n",
              "      <td>57.482484</td>\n",
              "      <td>Train</td>\n",
              "      <td>5.470862</td>\n",
              "      <td>0.312855</td>\n",
              "      <td>1.454695</td>\n",
              "      <td>0.786528</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-223</td>\n",
              "      <td>-201</td>\n",
              "      <td>-193</td>\n",
              "      <td>-160</td>\n",
              "      <td>-223</td>\n",
              "      <td>-227</td>\n",
              "      <td>-193</td>\n",
              "      <td>-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276.875</td>\n",
              "      <td>98.976088</td>\n",
              "      <td>6.0033</td>\n",
              "      <td>3.9206</td>\n",
              "      <td>1.054669</td>\n",
              "      <td>1.483483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-05-01 05:00:00</th>\n",
              "      <td>12483</td>\n",
              "      <td>WF3</td>\n",
              "      <td>4.845900</td>\n",
              "      <td>0.702200</td>\n",
              "      <td>1.011645</td>\n",
              "      <td>0.785352</td>\n",
              "      <td>275.530</td>\n",
              "      <td>89.971463</td>\n",
              "      <td>Train</td>\n",
              "      <td>4.896512</td>\n",
              "      <td>0.143904</td>\n",
              "      <td>1.280704</td>\n",
              "      <td>0.660129</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>-223</td>\n",
              "      <td>-201</td>\n",
              "      <td>-193</td>\n",
              "      <td>-159</td>\n",
              "      <td>-223</td>\n",
              "      <td>-227</td>\n",
              "      <td>-193</td>\n",
              "      <td>-14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>276.875</td>\n",
              "      <td>98.976088</td>\n",
              "      <td>6.0033</td>\n",
              "      <td>3.9206</td>\n",
              "      <td>1.054669</td>\n",
              "      <td>1.483483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 96 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        ID  ... V_10m_Expanded_Window_Max\n",
              "Time                        ...                          \n",
              "2018-05-01 01:00:00  12479  ...                  1.317597\n",
              "2018-05-01 02:00:00  12480  ...                  1.483483\n",
              "2018-05-01 03:00:00  12481  ...                  1.483483\n",
              "2018-05-01 04:00:00  12482  ...                  1.483483\n",
              "2018-05-01 05:00:00  12483  ...                  1.483483\n",
              "\n",
              "[5 rows x 96 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDPihT4eMaUt",
        "colab_type": "text"
      },
      "source": [
        "## Scaling Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7eE4ZXdbyU",
        "colab_type": "text"
      },
      "source": [
        "For a better performance of the Network, here the data is scaled between [-1,1] using MinMaxScaler. For the Direction Data, presented in degrees, Sin and Cos are calculated, which naturally have values in this same scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpn10ECJG97j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_X(X):\n",
        "\n",
        "  scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "\n",
        "  X_saved_columns = X[['ID','WF','Set','Month_Number']]\n",
        "  X_saved_columns = X_saved_columns.reset_index().drop('Time',axis=1)\n",
        "  X = X.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "\n",
        "  # Fill NaN's\n",
        "  #X = X.fillna(method=\"ffill\", axis=1) # ZOH\n",
        "  #X = X.fillna(0)\n",
        "\n",
        "  # Scaling Data\n",
        "  directions = X[['Wind Direction 100m', 'Wind Direction 10m']]\n",
        "  directions[\"Sin_Wind Direction 100m\"] = np.sin(X['Wind Direction 100m']*(np.pi/180))\n",
        "  directions[\"Cos_Wind Direction 100m\"] = np.cos(X['Wind Direction 100m']*(np.pi/180))\n",
        "  directions[\"Sin_Wind Direction 10m\"] = np.sin(X['Wind Direction 10m']*(np.pi/180))\n",
        "  directions[\"Cos_Wind Direction 10m\"] = np.cos(X['Wind Direction 10m']*(np.pi/180))\n",
        "  directions = directions.drop(['Wind Direction 100m', 'Wind Direction 10m'],axis=1)\n",
        "  directions = directions.reset_index().drop('Time',axis=1)\n",
        "\n",
        "  X = X.drop(['Wind Direction 100m', 'Wind Direction 10m'],axis=1)\n",
        "  X_columns = X.columns\n",
        "\n",
        "  X = scaler.fit_transform(X)\n",
        "  X = pd.DataFrame(X,columns=X_columns)\n",
        "  X = pd.concat([X,directions],axis=1)\n",
        "  X = pd.concat([X,X_saved_columns],axis=1)\n",
        "\n",
        "  X = X.fillna(-2)\n",
        "\n",
        "  return X"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b95dMH0BHFmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_y(y):\n",
        "\n",
        "  scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "\n",
        "  #y = y.fillna(method=\"ffill\", axis=1) # ZOH\n",
        "  #y = y.fillna(0)\n",
        "  y = y.drop('ID',axis=1)\n",
        "  y = scaler.fit_transform(y)\n",
        "  y = pd.DataFrame(y)\n",
        "  y = y.fillna(-2)\n",
        "\n",
        "  return y,scaler"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UreNMlu_UmBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "299db4b8-3876-4c1f-d9a8-ceb31af00c80"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6239, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOe3UyUzMaU_",
        "colab_type": "text"
      },
      "source": [
        "## Subsets Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdZ3SWMKMaU_",
        "colab_type": "text"
      },
      "source": [
        "Here, the Data is converted to a group of subsets where each subset has n_steps of past data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5ns8yzXMaU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 60"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A800bL_SMaVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split a multivariate sequence into samples\n",
        "def split_sequences(X, y = None, n_steps = 1):\n",
        "\tsample_X, sample_y = list(), list()\n",
        "\tfor i in range(len(X)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif end_ix > len(X):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x = X[i:end_ix, :]\n",
        "\t\tsample_X.append(seq_x)\n",
        "\t\tif y is not None:\n",
        "\t\t\tseq_y = y[end_ix-1,-1]\n",
        "\t\t\tsample_y.append(seq_y)\n",
        "\treturn np.array(sample_X), np.array(sample_y)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okz8xasuzDXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shift_save(df,n_steps):\n",
        "  empty = pd.DataFrame(np.zeros((n_steps-1,df.shape[1])),columns=df.columns)\n",
        "  df = pd.concat([empty,df])\n",
        "  return df"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgXPLQ0MaVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_X,sample_y = split_sequences(X.values,y.values,n_steps)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qSIjG8HMaVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_features = sample_X.shape[2] - 2"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzkqvqX5MaVJ",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USm2oSuSMaVK",
        "colab_type": "text"
      },
      "source": [
        "Here, a function to create the Model usin Keras is defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt8xYhd3MaVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM_Model(input_shape, batch_size=1):\n",
        "  # Numerical branch\n",
        "\n",
        "  input_layer = tf.keras.Input(shape = input_shape,batch_size = batch_size)\n",
        "\n",
        "  hidden_1 = tf.keras.layers.LSTM(units=128,return_sequences=True,stateful=True)(input_layer)\n",
        "  hidden_1 = tf.keras.layers.Dropout(0.4)(hidden_1)\n",
        "\n",
        "  hidden_2 = tf.keras.layers.LSTM(units=87,return_sequences=True,stateful=True)(hidden_1)\n",
        "  hidden_2 = tf.keras.layers.Dropout(0.4)(hidden_2)\n",
        "\n",
        "  hidden_3 = tf.keras.layers.LSTM(units=57,stateful=True)(hidden_2)\n",
        "  hidden_3 = tf.keras.layers.Dropout(0.4)(hidden_3)\n",
        "\n",
        "  # Output\n",
        "  #outputs = tf.keras.layers.PReLU()(hidden_2)\n",
        "  #outputs = tf.keras.layers.Dropout(rate=0.2)(hidden_2)\n",
        "  outputs = tf.keras.layers.Dense(units=1)(hidden_3)\n",
        "\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrjTyZ8KMaVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = (n_steps,n_features)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ja7P7f1HMaVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM_Model(input_shape)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "__ARBNVBMaVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "93771583-8f3d-4939-f7ee-41370d805a3b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_19 (InputLayer)        [(1, 60, 94)]             0         \n",
            "_________________________________________________________________\n",
            "lstm_58 (LSTM)               (1, 60, 128)              114176    \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (1, 60, 128)              0         \n",
            "_________________________________________________________________\n",
            "lstm_59 (LSTM)               (1, 60, 87)               75168     \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (1, 60, 87)               0         \n",
            "_________________________________________________________________\n",
            "lstm_60 (LSTM)               (1, 57)                   33060     \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (1, 57)                   0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (1, 1)                    58        \n",
            "=================================================================\n",
            "Total params: 222,462\n",
            "Trainable params: 222,462\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpR1xJhqMaVT",
        "colab_type": "text"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4K8mWyxMaVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(317)\n",
        "tf.random.set_seed(317)\n",
        "\n",
        "patience = 3\n",
        "epochs = 10\n",
        "k_fold_splits = 5\n",
        "total_it = 50\n",
        "monitor = \"root_mean_squared_error\"\n",
        "batch_size = 1"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqDvZgJusJ6R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebb5e9aa-3a55-411d-e52c-a3dcaca7f4bf"
      },
      "source": [
        "# Define Time Split Cross Validation\n",
        "tscv = TimeSeriesSplit(n_splits=k_fold_splits)\n",
        "\n",
        "# Separating Data from Hold Out Set\n",
        "\n",
        "X_cv, _, y_cv, _ = train_test_split(X, y, test_size=0.125, shuffle=False)\n",
        "\n",
        "train_scores = np.empty(0)\n",
        "val_scores = np.empty(0)\n",
        "test_scores = np.empty(0)\n",
        "for train_index, test_index in tscv.split(X_cv):\n",
        "\n",
        "    #train_index = train_index[-n_rows:]\n",
        "    #test_index = test_index[-n_rows:]\n",
        "\n",
        "    # Get the Data of the Split\n",
        "    X_train, X_test = X_cv.iloc[train_index], X_cv.iloc[test_index]\n",
        "    y_train, y_test = y_cv.iloc[train_index], y_cv.iloc[test_index]\n",
        "\n",
        "    # Separating Training Set of Split on Train and Validation Subsets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)\n",
        "\n",
        "    # Preprocessing Data\n",
        "    X_train = preprocessing_X(X_train)\n",
        "    X_val = preprocessing_X(X_val)\n",
        "    X_test = preprocessing_X(X_test)\n",
        "\n",
        "    X_train = X_train.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "    X_val = X_val.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "    X_test = X_test.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "\n",
        "    y_train,_ = preprocessing_y(y_train)\n",
        "    y_val,_ = preprocessing_y(y_val)\n",
        "    y_test,_ = preprocessing_y(y_test)\n",
        "\n",
        "    # Reshape Data\n",
        "    X_train, y_train = split_sequences(X_train.values,y_train.values,n_steps)\n",
        "    X_val, y_val = split_sequences(X_val.values,y_val.values,n_steps)\n",
        "    X_test, y_test = split_sequences(X_test.values,y_test.values,n_steps)\n",
        "\n",
        "    # Create Model\n",
        "    model = LSTM_Model(input_shape,batch_size=batch_size)\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=0, min_delta=1e-8)]\n",
        "\n",
        "    # Train the Model\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    history = model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, y_val), callbacks=callbacks_list,shuffle=False)\n",
        "\n",
        "    # Train and Validation Score\n",
        "    train_score = np.array(history.history['root_mean_squared_error']).mean()\n",
        "    val_score = np.array(history.history['val_root_mean_squared_error']).mean()\n",
        "\n",
        "    # Test Score\n",
        "    preds = model.predict(X_test,batch_size = batch_size,callbacks=callbacks_list)\n",
        "    preds = tf.cast(preds, tf.float32)\n",
        "    y_test = tf.cast(y_test, tf.float32)\n",
        "\n",
        "    m = tf.keras.metrics.RootMeanSquaredError()\n",
        "    m.update_state(y_test,preds)\n",
        "    test_score = m.result().numpy()\n",
        "\n",
        "    train_scores = np.append(train_scores,train_score)\n",
        "    val_scores = np.append(val_scores,val_score)\n",
        "    test_scores = np.append(test_scores,test_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "724/724 [==============================] - 8s 11ms/step - loss: 0.1552 - root_mean_squared_error: 0.3940 - val_loss: 0.1952 - val_root_mean_squared_error: 0.4419\n",
            "Epoch 2/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.1403 - root_mean_squared_error: 0.3746 - val_loss: 0.1958 - val_root_mean_squared_error: 0.4425\n",
            "Epoch 3/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.1381 - root_mean_squared_error: 0.3716 - val_loss: 0.1572 - val_root_mean_squared_error: 0.3964\n",
            "Epoch 4/10\n",
            "724/724 [==============================] - 7s 9ms/step - loss: 0.1342 - root_mean_squared_error: 0.3664 - val_loss: 0.1363 - val_root_mean_squared_error: 0.3692\n",
            "Epoch 5/10\n",
            " 75/724 [==>...........................] - ETA: 6s - loss: 0.1578 - root_mean_squared_error: 0.3973"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA7bnLfksZLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(range(len(train_scores)),train_scores,label='Train Scores')\n",
        "plt.plot(range(len(val_scores)),val_scores,label='Validation Scores')\n",
        "plt.plot(range(len(test_scores)),test_scores,label='Test Scores')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDJ3EJABMaVe",
        "colab_type": "text"
      },
      "source": [
        "### Hold Out Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrZ9HNlNcSLE",
        "colab_type": "text"
      },
      "source": [
        "Here, the same model is trained on all the data used on the Validation and tested on a Holdout Set never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMcTgPbPMaVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.125, shuffle=False)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.143, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv6-ih-IIK3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = preprocessing_X(X_train)\n",
        "X_val = preprocessing_X(X_val)\n",
        "X_holdout = preprocessing_X(X_holdout)\n",
        "\n",
        "X_train = X_train.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "X_val = X_val.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "X_holdout = X_holdout.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "\n",
        "y_train,scaler = preprocessing_y(y_train)\n",
        "y_val,_ = preprocessing_y(y_val)\n",
        "y_holdout,_ = preprocessing_y(y_holdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCt5uDOTMaVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = split_sequences(X_train.values,y_train.values,n_steps)\n",
        "X_val, y_val = split_sequences(X_val.values,y_val.values,n_steps)\n",
        "X_holdout, y_holdout = split_sequences(X_holdout.values,y_holdout.values,n_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLZJNzIrMaVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LSTM_Model(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "XUfPcqXlMaVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=0, min_delta=1e-8)]\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, y_val), callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "X4PNtrv0MaVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(X_holdout,batch_size = batch_size,callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8PCgJ71cDUf",
        "colab_type": "text"
      },
      "source": [
        "Here, Predicitions and True Labels of the Holdout Set are scaled back to measure the CAPE Error, and be compared on a Chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXT215AXZ1mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = scaler.inverse_transform(preds)\n",
        "y_holdout = scaler.inverse_transform(pd.DataFrame(y_holdout))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnu0r9nEsdN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_cnr(preds,labels):\n",
        "    cape_cnr = 100*np.sum(np.abs(preds-labels))/np.sum(labels)\n",
        "    return 'CAPE', cape_cnr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2_xB0ODWiZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_cnr(preds,y_holdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TjOBo0MaVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "preds_len = np.arange(len(y_holdout))\n",
        "plt.plot(preds_len,y_holdout,label='True Values')\n",
        "plt.plot(preds_len,preds,'r--',label='Predicts')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-CCiRv1Mias",
        "colab_type": "text"
      },
      "source": [
        "## Submission Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAz3_aaLRNcG",
        "colab_type": "text"
      },
      "source": [
        "First, a function with all the preprocessing made to prepare data for network is defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYtIDxev6ave",
        "colab_type": "text"
      },
      "source": [
        "Here's a Function with all the Process of Submit Gen for One Wind Farm. This Model is the Standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgk0VwBB-cVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WF_submit_gen(full_data,full_label,WF,input_shape,batch_size):\n",
        "  # Get Data of Wind Farm\n",
        "  X_WF = full_data[full_data['WF']==WF]\n",
        "\n",
        "  # Scale X Data\n",
        "  X_WF = preprocessing_X(X_WF)\n",
        "\n",
        "  full_X_train = X_WF[X_WF['Set']=='Train']\n",
        "  full_X_test = X_WF[X_WF['Set']=='Test']\n",
        "  full_y_train = full_label[full_label['ID'].isin(full_X_train['ID'])]\n",
        "\n",
        "  # Scale y_train\n",
        "  full_y_train,scaler_y = preprocessing_y(full_y_train)\n",
        "\n",
        "  # Subsets Generation\n",
        "  full_X_train = full_X_train.drop(['ID','WF','Set'],axis=1)\n",
        "  full_X_train, full_y_train = split_sequences(full_X_train.values,full_y_train.values,n_steps)\n",
        "\n",
        "  full_X_test = full_X_test.reset_index().drop('index',axis=1)\n",
        "  full_X_test_split = full_X_test.drop(['ID','WF','Set'],axis=1)\n",
        "  full_X_test_split = shift_save(full_X_test_split,n_steps)\n",
        "  full_X_test_split, _ = split_sequences(full_X_test_split.values,None,n_steps)\n",
        "\n",
        "  # Months Loop\n",
        "  WF_preds = np.empty(0)\n",
        "  predicted_X_test = np.empty((0,60,95))\n",
        "  for month in full_X_test['Month_Number'].unique():\n",
        "\n",
        "    print(\"Farm {} - Month {}\".format(WF,month))\n",
        "\n",
        "    # Special Condition for Month 10\n",
        "    if month == 9:\n",
        "      X_test = full_X_test[full_X_test['Month_Number']==month]\n",
        "      X_test_10 = full_X_test[full_X_test['Month_Number']==10]\n",
        "      X_test = pd.concat([X_test,X_test_10])\n",
        "    else:\n",
        "      X_test = full_X_test[full_X_test['Month_Number']==month]\n",
        "\n",
        "    if month == 10:\n",
        "      continue\n",
        "    \n",
        "    # Separate Month Data on Numpy Matrix\n",
        "    ids = X_test.index\n",
        "    X_test = full_X_test_split[[ids]]\n",
        "\n",
        "    # Append Data Already Predicted\n",
        "    X_train = np.append(full_X_train,predicted_X_test,axis=0)\n",
        "    y_train = np.append(full_y_train,WF_preds,axis=0)\n",
        "\n",
        "    # Split Train Data on Train and Validation Set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Model Creation and Training\n",
        "    model = LSTM_Model(input_shape,batch_size=batch_size)\n",
        "    callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=0, min_delta=1e-8)]\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, y_val), callbacks=callbacks_list,shuffle=False)\n",
        "\n",
        "    # Prediction Generation\n",
        "    pred = model.predict(X_test,batch_size = batch_size,callbacks=callbacks_list)\n",
        "    pred = pred.reshape(pred.shape[0])\n",
        "\n",
        "    # Save Predictions on Final Array\n",
        "    predicted_X_test = np.append(predicted_X_test,X_test,axis=0)\n",
        "    WF_preds = np.append(WF_preds,pred)\n",
        "\n",
        "  WF_preds = scaler_y.inverse_transform(pd.DataFrame(WF_preds))\n",
        "  WF_preds = pd.DataFrame(WF_preds,columns=['Production'])\n",
        "\n",
        "  return WF_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Y2kRnDLxG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WF_submit_gen_2(full_data,full_label,WF,input_shape,batch_size):\n",
        "  # Get Data of Wind Farm\n",
        "  X_WF = full_data[full_data['WF']==WF]\n",
        "\n",
        "  full_X_train = X_WF[X_WF['Set']=='Train']\n",
        "  full_X_test = X_WF[X_WF['Set']=='Test']\n",
        "  full_y_train = full_label[full_label['ID'].isin(full_X_train['ID'])]\n",
        "\n",
        "  # Months Loop\n",
        "  WF_preds = pd.DataFrame(columns=['Production'])\n",
        "  predicted_X_test = pd.DataFrame(columns=full_X_train.columns)\n",
        "  predicted_X_test.index.names = ['Time']\n",
        "  for month in full_X_test['Month_Number'].unique():\n",
        "\n",
        "    print(\"Farm {} - Month {}\".format(WF,month))\n",
        "\n",
        "    # Special Condition for Month 10\n",
        "    if month == 9:\n",
        "      X_test = full_X_test[full_X_test['Month_Number']==month]\n",
        "      X_test_10 = full_X_test[full_X_test['Month_Number']==10]\n",
        "      X_test_pure = pd.concat([X_test,X_test_10])\n",
        "    else:\n",
        "      X_test_pure = full_X_test[full_X_test['Month_Number']==month]\n",
        "\n",
        "    if month == 10:\n",
        "      continue\n",
        "\n",
        "    # Append Data Already Predicted\n",
        "    X_train = pd.concat([full_X_train,predicted_X_test])\n",
        "    y_train = pd.concat([full_y_train,WF_preds])\n",
        "\n",
        "    # Split Train Data on Train and Validation Set\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Preprocessing Data\n",
        "    X_train = preprocessing_X(X_train)\n",
        "    X_val = preprocessing_X(X_val)\n",
        "    X_test = preprocessing_X(X_test_pure)\n",
        "\n",
        "    # Shift-Save on X_test\n",
        "    X_test = shift_save(X_test,n_steps)\n",
        "\n",
        "    X_train = X_train.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "    X_val = X_val.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "    X_test = X_test.drop(['ID','WF','Set','Month_Number'],axis=1)\n",
        "\n",
        "    y_train,scaler_y = preprocessing_y(y_train)\n",
        "    y_val,_ = preprocessing_y(y_val)\n",
        "\n",
        "    # Subset Creation \n",
        "\n",
        "    X_train, y_train = split_sequences(X_train.values,y_train.values,n_steps)\n",
        "    X_val, y_val = split_sequences(X_val.values,y_val.values,n_steps)\n",
        "    X_test, _ = split_sequences(X_test.values,None,n_steps)\n",
        "\n",
        "    # Model Creation and Training\n",
        "    model = LSTM_Model(input_shape,batch_size=batch_size)\n",
        "    callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=0, min_delta=1e-8)]\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, y_val), callbacks=callbacks_list,shuffle=False)\n",
        "\n",
        "    # Prediction Generation\n",
        "    pred = model.predict(X_test,batch_size = batch_size,callbacks=callbacks_list)\n",
        "    pred = pred.reshape(pred.shape[0])\n",
        "    pred = pd.DataFrame(pred,columns=['Production'])\n",
        "\n",
        "    # Save Predictions on Final Array\n",
        "    predicted_X_test = pd.concat([predicted_X_test,X_test_pure])\n",
        "\n",
        "    pred = scaler_y.inverse_transform(pred)\n",
        "    pred = pd.DataFrame(pred,columns=['Production'])\n",
        "    WF_preds = pd.concat([WF_preds,pred])\n",
        "\n",
        "  #WF_preds = scaler_y.inverse_transform(pd.DataFrame(WF_preds))\n",
        "  #WF_preds = pd.DataFrame(WF_preds,columns=['Production'])\n",
        "\n",
        "  return WF_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1_odnSc6lLx",
        "colab_type": "text"
      },
      "source": [
        "Direct Submit Generation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZxI1nocllLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WF_submit_gen_3(full_data,full_label,WF,input_shape,batch_size):\n",
        "  # Get Data of Wind Farm\n",
        "  X_WF = full_data[full_data['WF']==WF]\n",
        "\n",
        "  # Scale X Data\n",
        "  X_WF = preprocessing_X(X_WF)\n",
        "\n",
        "  full_X_train = X_WF[X_WF['Set']=='Train']\n",
        "  full_X_test = X_WF[X_WF['Set']=='Test']\n",
        "  full_y_train = full_label[full_label['ID'].isin(full_X_train['ID'])]\n",
        "\n",
        "  # Scale y_train\n",
        "  full_y_train,scaler_y = preprocessing_y(full_y_train)\n",
        "\n",
        "  # Subsets Generation\n",
        "  full_X_train = full_X_train.drop(['ID','WF','Set'],axis=1)\n",
        "  full_X_train, full_y_train = split_sequences(full_X_train.values,full_y_train.values,n_steps)\n",
        "\n",
        "  full_X_test_split = full_X_test.drop(['ID','WF','Set'],axis=1)\n",
        "  full_X_test_split = shift_save(full_X_test_split,n_steps)\n",
        "  full_X_test_split, _ = split_sequences(full_X_test_split.values,None,n_steps)\n",
        "\n",
        "  # Split Train Data on Train and Validation Set\n",
        "  X_train, X_val, y_train, y_val = train_test_split(full_X_train, full_y_train, test_size=0.3, shuffle=False)\n",
        "\n",
        "  # Model Creation and Training\n",
        "  model = LSTM_Model(input_shape,batch_size=batch_size)\n",
        "  callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor=monitor, patience=patience, verbose=0, min_delta=1e-8)]\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "  model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = epochs, validation_data = (X_val, y_val), callbacks=callbacks_list,shuffle=False)\n",
        "\n",
        "  # Prediction Generation\n",
        "  pred = model.predict(full_X_test_split,batch_size = batch_size,callbacks=callbacks_list)\n",
        "  pred = pred.reshape(pred.shape[0])\n",
        "\n",
        "  WF_preds = scaler_y.inverse_transform(pd.DataFrame(pred))\n",
        "  WF_preds = pd.DataFrame(WF_preds,columns=['Production'])\n",
        "\n",
        "  return WF_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhdQ4elWhebU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_preds = pd.DataFrame()\n",
        "for WF in full_data['WF'].unique():\n",
        "  WF_preds = WF_submit_gen_2(full_data,full_label,WF,input_shape,batch_size)\n",
        "  final_preds = final_preds.append(WF_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu6q2KZZNR3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_preds = final_preds.reset_index().drop('index',axis=1)\n",
        "final_preds['ID'] = pd.read_csv(r'random_submission_example.csv')['ID']\n",
        "final_preds = final_preds.set_index('ID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGnL_N8tNGlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XMZasjfNVhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(final_preds.index,final_preds['Production'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvmNmz_tNXPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_preds.to_csv('Submission_LSTM.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhoZvdSu_JL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('Submission_LSTM.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5S97fCFKlCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(final_preds[final_preds['Production'] > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80XS7C47KyNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}